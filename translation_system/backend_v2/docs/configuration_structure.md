# Backend V2 é…ç½®ç»“æ„è¯´æ˜

## ğŸ”§ é…ç½®åˆ†ç¦»è®¾è®¡

ä¸ºäº†åŒºåˆ†**ä»»åŠ¡æ‹†è§£æ§åˆ¶**å’Œ**LLM APIé…ç½®**ï¼Œæˆ‘ä»¬é‡æ–°è®¾è®¡äº†é…ç½®ç»“æ„ï¼š

### ğŸ“Š 1. ä»»åŠ¡æ‰§è¡Œæ§åˆ¶ (`task_execution`)

**ç”¨é€”**: æ§åˆ¶ä»»åŠ¡æ‹†è§£ã€æ‰¹æ¬¡åˆ†ç»„ã€å¹¶å‘æ‰§è¡Œç­‰ä¸šåŠ¡é€»è¾‘

```yaml
task_execution:
  # æ‰¹æ¬¡æ§åˆ¶ - ç”¨äºä»»åŠ¡åˆ†ç»„å’Œæ‰§è¡Œæ§åˆ¶
  batch_control:
    max_chars_per_batch: 1000      # æ¯æ‰¹æ¬¡æœ€å¤§å­—ç¬¦æ•°ï¼ˆç”¨äºä»»åŠ¡æ‹†è§£ï¼‰
    max_concurrent_workers: 10     # æœ€å¤§å¹¶å‘workeræ•°

  # æ‹†è§£æ§åˆ¶ - ç”¨äºä»»åŠ¡åˆ†å‰²
  split_control:
    max_task_chars: 500           # å•ä¸ªä»»åŠ¡æœ€å¤§å­—ç¬¦æ•°
    context_overlap: 50           # ä¸Šä¸‹æ–‡é‡å å­—ç¬¦æ•°
    min_batch_size: 1             # æœ€å°æ‰¹æ¬¡å¤§å°
```

### ğŸ¤– 2. LLM APIé…ç½® (`llm`)

**ç”¨é€”**: æ§åˆ¶LLM APIè°ƒç”¨å‚æ•°ï¼ŒåŒ…æ‹¬tokené™åˆ¶ã€æ¨¡å‹è®¾ç½®ç­‰

```yaml
llm:
  default_provider: qwen-plus

  providers:
    qwen-plus:
      enabled: true
      api_key: "sk-xxx"
      base_url: "https://dashscope.aliyuncs.com/api/v1"
      model: "qwen-plus"
      temperature: 0.3
      max_tokens: 8000              # LLM APIæœ€å¤§è¾“å‡ºtokenæ•°ï¼ˆæ¯”ä»»åŠ¡æ§åˆ¶å¤§å¾ˆå¤šï¼‰
      timeout: 90
      max_retries: 3
      retry_delay: 3.0
```

## ğŸ” å…³é”®å·®å¼‚å¯¹æ¯”

| é…ç½®é¡¹ | æ—§ç»“æ„ | æ–°ç»“æ„ | ç”¨é€” | å…¸å‹å€¼ |
|--------|--------|--------|------|--------|
| **ä»»åŠ¡æ‹†è§£æ§åˆ¶** | `llm.batch_control.max_chars_per_batch` | `task_execution.batch_control.max_chars_per_batch` | æ§åˆ¶ä»»åŠ¡åˆ†ç»„å¤§å° | 1000 |
| **LLM Tokené™åˆ¶** | `llm.providers.*.max_tokens` | `llm.providers.*.max_tokens` | LLM APIè¾“å‡ºé™åˆ¶ | 8000 |
| **å•ä»»åŠ¡å¤§å°** | âŒ æœªé…ç½® | `task_execution.split_control.max_task_chars` | å•ä¸ªä»»åŠ¡å­—ç¬¦é™åˆ¶ | 500 |
| **å¹¶å‘æ§åˆ¶** | `llm.batch_control.max_concurrent_workers` | `task_execution.batch_control.max_concurrent_workers` | Workerå¹¶å‘æ•° | 10 |

## ğŸ“ ä»£ç ä½¿ç”¨ç¤ºä¾‹

### 1. è·å–ä»»åŠ¡æ‰§è¡Œæ§åˆ¶å€¼

```python
from utils.config_manager import config_manager

# è·å–ä»»åŠ¡æ‹†è§£ç›¸å…³é…ç½®
max_chars_per_batch = config_manager.max_chars_per_batch        # 1000
max_concurrent_workers = config_manager.max_concurrent_workers  # 10
max_task_chars = config_manager.max_task_chars                 # 500
context_overlap = config_manager.context_overlap              # 50
```

### 2. è·å–LLM APIé…ç½®

```python
from services.llm.llm_factory import LLMFactory

# åˆ›å»ºLLM Provideræ—¶ä¼šè‡ªåŠ¨è¯»å–max_tokensç­‰é…ç½®
provider = LLMFactory.create_from_config_file(config, 'qwen-plus')
# provider.config.max_tokens = 8000ï¼ˆä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰
```

## ğŸ¯ é…ç½®åŸç†è¯´æ˜

### ä¸ºä»€ä¹ˆéœ€è¦åˆ†ç¦»ï¼Ÿ

1. **ä¸šåŠ¡é€»è¾‘ä¸åŒ**:
   - `max_chars_per_batch` (1000): ç”¨äºæ§åˆ¶**ä¸šåŠ¡æ‰¹æ¬¡**å¤§å°ï¼Œé¿å…å•æ‰¹æ¬¡ä»»åŠ¡è¿‡å¤š
   - `max_tokens` (8000): ç”¨äºæ§åˆ¶**LLM API**çš„è¾“å‡ºé•¿åº¦ï¼ŒæŠ€æœ¯é™åˆ¶

2. **æ•°å€¼å·®å¼‚å·¨å¤§**:
   - ä»»åŠ¡æ§åˆ¶å€¼é€šå¸¸è¾ƒå° (500-1000 å­—ç¬¦)
   - LLM tokené™åˆ¶è¾ƒå¤§ (4000-8000 tokens)

3. **é…ç½®ç‹¬ç«‹æ€§**:
   - ä»»åŠ¡æ‰§è¡Œå‚æ•°: å½±å“æ€§èƒ½å’Œèµ„æºä½¿ç”¨
   - LLM APIå‚æ•°: å½±å“ç¿»è¯‘è´¨é‡å’Œæˆæœ¬

### å®é™…ä½¿ç”¨åœºæ™¯

```
ä»»åŠ¡æ‹†è§£æµç¨‹:
åŸå§‹æ–‡æœ¬(5000å­—ç¬¦)
â†’ æŒ‰max_task_chars(500)æ‹†åˆ†æˆ10ä¸ªä»»åŠ¡
â†’ æŒ‰max_chars_per_batch(1000)åˆ†æˆ5ä¸ªæ‰¹æ¬¡
â†’ æ¯æ‰¹æ¬¡å¹¶å‘è°ƒç”¨LLM
â†’ LLMä½¿ç”¨max_tokens(8000)é™åˆ¶è¾“å‡ºé•¿åº¦
```

## âš¡ æ€§èƒ½è°ƒä¼˜å»ºè®®

### 1. ä»»åŠ¡æ‰§è¡Œä¼˜åŒ–
- `max_chars_per_batch`: 1000-5000 (æ ¹æ®å†…å­˜å’Œæ€§èƒ½è°ƒæ•´)
- `max_concurrent_workers`: 5-20 (æ ¹æ®APIé™åˆ¶è°ƒæ•´)
- `max_task_chars`: 300-800 (æ ¹æ®ç¿»è¯‘ç²¾åº¦è°ƒæ•´)

### 2. LLM APIä¼˜åŒ–
- `max_tokens`: 4000-8000 (æ ¹æ®æ¨¡å‹é™åˆ¶è®¾ç½®)
- `temperature`: 0.1-0.5 (ç¿»è¯‘ä»»åŠ¡å»ºè®®è¾ƒä½å€¼)
- `timeout`: 60-120 (æ ¹æ®ç½‘ç»œæƒ…å†µè°ƒæ•´)

è¿™æ ·çš„é…ç½®åˆ†ç¦»ä½¿å¾—ç³»ç»Ÿæ›´åŠ æ¸…æ™°ï¼Œä¾¿äºè°ƒä¼˜å’Œç»´æŠ¤ã€‚