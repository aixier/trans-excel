# 第二阶段任务列表 - 执行引擎与监控

## 阶段目标
实现LLM翻译执行引擎，支持并发批次执行，提供实时监控接口

## 任务列表

| 序号 | 状态 | 任务名称 | 目标 | 参考文件 | 依赖 |
|------|------|----------|------|----------|------|
| 2.1 | ✅ | LLM Provider基础类 | 定义LLM提供商抽象接口，支持多种LLM | `services/llm/base_provider.py` | - |
| 2.2 | ✅ | OpenAI Provider实现 | 实现OpenAI GPT-4接口，支持批量翻译 | `services/llm/openai_provider.py` | 2.1 |
| 2.3 | ✅ | 阿里云通义千问Provider | 实现Qwen接口，支持批量翻译 | `services/llm/qwen_provider.py` | 2.1 |
| 2.4 | ✅ | LLM Factory工厂类 | 根据配置动态创建LLM实例 | `services/llm/llm_factory.py` | 2.1, 2.2, 2.3 |
| 2.5 | ✅ | 翻译Prompt模板 | 设计游戏翻译专用prompt模板，支持上下文注入 | `services/llm/prompt_template.py` | - |
| 2.6 | ✅ | 批次执行器 | 实现BatchExecutor，处理单个批次的翻译任务 | `services/executor/batch_executor.py` | 2.4, 2.5 |
| 2.7 | ✅ | Worker池管理器 | 实现WorkerPool，并发执行多个批次 | `services/executor/worker_pool.py` | 2.6 |
| 2.8 | ✅ | 执行控制API | POST /api/execute/start 启动执行 | `api/execute_api.py` | 2.7 |
| 2.9 | ✅ | 执行停止API | POST /api/execute/stop 停止执行 | `api/execute_api.py` | 2.7 |
| 2.10 | ✅ | 进度监控API | GET /api/monitor/status 获取实时进度 | `api/monitor_api.py` | 2.7 |
| 2.11 | ✅ | DataFrame查询API | GET /api/monitor/dataframe 查询任务详情 | `api/monitor_api.py` | - |
| 2.12 | ✅ | 错误重试机制 | 实现失败任务自动重试，最多3次 | `services/executor/batch_executor.py` | 2.6 |
| 2.13 | ✅ | 执行日志服务 | 记录LLM调用日志，包括耗时、token使用等 | `services/llm/base_provider.py` | 2.6 |
| 2.14 | ✅ | 成本统计服务 | 统计token使用量和API调用成本 | `services/executor/batch_executor.py` | 2.13 |
| 2.15 | ⚠️ | WebSocket实时推送 | 实现进度实时推送（可选） | - | 2.10 |

## 详细设计

### 2.1 LLM Provider基础类
```python
# services/llm/base_provider.py
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

@dataclass
class TranslationRequest:
    source_text: str
    source_lang: str
    target_lang: str
    context: str
    game_info: Dict[str, Any]

@dataclass
class TranslationResponse:
    translated_text: str
    confidence: float
    token_usage: Dict[str, int]
    model: str
    duration_ms: int

class BaseLLMProvider(ABC):
    @abstractmethod
    async def translate_batch(
        self,
        requests: List[TranslationRequest]
    ) -> List[TranslationResponse]:
        pass
```

### 2.2-2.3 LLM Provider实现
```python
# services/llm/openai_provider.py
class OpenAIProvider(BaseLLMProvider):
    def __init__(self, config: Dict[str, Any]):
        self.api_key = config['api_key']
        self.model = config.get('model', 'gpt-4')
        self.temperature = config.get('temperature', 0.3)

# services/llm/qwen_provider.py
class QwenProvider(BaseLLMProvider):
    def __init__(self, config: Dict[str, Any]):
        self.api_key = config['api_key']
        self.model = config.get('model', 'qwen-max')
```

### 2.5 翻译Prompt模板
```python
# services/llm/prompt_template.py
GAME_TRANSLATION_PROMPT = """
你是一名专业的游戏翻译专家。请根据以下信息进行翻译：

游戏信息:
- 类型: {game_type}
- 世界观: {world_view}
- 风格: {game_style}

上下文信息:
{context}

请将以下{source_lang}文本翻译成{target_lang}：
{source_text}

要求：
1. 保持游戏术语的一致性
2. 符合目标地区的文化习惯
3. 保留特殊标记和变量（如{0}, %s等）
4. 注意UI文本长度限制

只返回翻译后的文本，不要包含其他解释。
"""
```

### 2.6 批次执行器
```python
# services/executor/batch_executor.py
class BatchExecutor:
    def __init__(self, llm_provider: BaseLLMProvider):
        self.llm_provider = llm_provider

    async def execute_batch(
        self,
        batch_id: str,
        tasks: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """执行单个批次的翻译任务"""
        # 1. 构建TranslationRequest列表
        # 2. 调用LLM provider
        # 3. 更新任务状态和结果
        # 4. 返回更新后的任务列表
```

### 2.7 Worker池管理器
```python
# services/executor/worker_pool.py
import asyncio
from typing import List, Dict, Any

class WorkerPool:
    def __init__(self, max_workers: int = 10):
        self.max_workers = max_workers
        self.active_workers = []
        self.queue = asyncio.Queue()

    async def start_execution(
        self,
        session_id: str
    ):
        """启动执行，创建worker处理任务"""
        # 1. 从TaskDataFrameManager获取待处理批次
        # 2. 创建worker协程
        # 3. 分配批次给worker
        # 4. 监控执行进度
```

### 2.8-2.9 执行控制API
```python
# api/execute_api.py
from fastapi import APIRouter, HTTPException
from services.executor.worker_pool import WorkerPool

router = APIRouter(prefix="/api/execute", tags=["execute"])

@router.post("/start")
async def start_execution(request: ExecuteRequest):
    """
    启动翻译执行
    - 读取session中的任务DataFrame
    - 启动Worker池
    - 返回执行状态
    """

@router.post("/stop/{session_id}")
async def stop_execution(session_id: str):
    """停止执行"""
```

### 2.10-2.11 监控API
```python
# api/monitor_api.py
@router.get("/status/{session_id}")
async def get_execution_status(session_id: str):
    """
    返回执行进度:
    {
        "progress": {
            "total": 150,
            "completed": 75,
            "processing": 10,
            "pending": 65,
            "failed": 0
        },
        "completion_rate": 50.0,
        "estimated_remaining_seconds": 120,
        "current_batches": ["BATCH_001", "BATCH_002"],
        "recent_completions": [...]
    }
    """

@router.get("/dataframe/{session_id}")
async def get_task_dataframe(
    session_id: str,
    status: Optional[str] = None,
    limit: int = 100,
    offset: int = 0
):
    """获取任务DataFrame内容，支持分页和状态过滤"""
```

## 配置文件更新
```yaml
# config/llm_config.yaml
llm:
  # 执行控制参数
  batch_control:
    max_chars_per_batch: 50000
    max_concurrent_workers: 10

  # LLM提供商配置
  providers:
    openai:
      enabled: true
      api_key: ${OPENAI_API_KEY}
      base_url: "https://api.openai.com/v1"
      model: "gpt-4-turbo-preview"
      temperature: 0.3
      max_tokens: 4000

    qwen:
      enabled: true
      api_key: ${QWEN_API_KEY}
      base_url: "https://dashscope.aliyuncs.com/api/v1"
      model: "qwen-max"
      temperature: 0.3

  # 默认提供商
  default_provider: "openai"

  # 重试配置
  retry:
    max_attempts: 3
    delay_seconds: 5
    exponential_backoff: true
```

## 测试计划

### 单元测试
1. **LLM Provider测试**
   - Mock API响应
   - 测试批量翻译功能
   - 测试错误处理

2. **批次执行器测试**
   - 测试单批次执行
   - 测试失败重试
   - 测试超时处理

### 集成测试
```python
# tests/test_execution_flow.py
async def test_complete_execution_flow():
    """测试完整执行流程"""
    # 1. 创建测试任务DataFrame
    # 2. 启动执行
    # 3. 监控进度
    # 4. 验证结果
```

## 依赖安装
```bash
pip install openai
pip install dashscope  # 阿里云SDK
pip install httpx
pip install tenacity  # 重试库
```

## 验收标准
- [ ] 能成功启动翻译执行
- [ ] Worker并发数符合配置(max_concurrent_workers=10)
- [ ] 批次大小符合配置(max_chars_per_batch=50000)
- [ ] 监控API返回实时进度
- [ ] DataFrame实时更新翻译结果
- [ ] 失败任务自动重试(最多3次)
- [ ] 记录详细执行日志
- [ ] 统计API调用成本

## 开发顺序建议
1. 先实现LLM Provider基础架构(2.1-2.4)
2. 实现简单的批次执行器(2.6)
3. 实现基础的执行API(2.8)
4. 添加监控功能(2.10-2.11)
5. 完善Worker池并发(2.7)
6. 添加错误处理和重试(2.12)
7. 最后添加日志和统计(2.13-2.14)

## 注意事项
1. **并发控制**: 严格控制max_concurrent_workers，避免超过API限制
2. **错误处理**: 单个任务失败不应影响整个批次
3. **内存管理**: DataFrame更新要及时，避免内存泄漏
4. **成本控制**: 记录每次API调用的token使用量
5. **断点续传**: 考虑执行中断后的恢复机制