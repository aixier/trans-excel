#!/usr/bin/env python3
"""
æ™ºèƒ½å¹¶å‘æ‰¹é‡ç¿»è¯‘ - ç®€åŒ–ç‰ˆ
- æ”¯æŒå‘½ä»¤è¡Œ -f å‚æ•°æŒ‡å®šè¾“å…¥æ–‡ä»¶
- æ™ºèƒ½è¯†åˆ«éœ€è¦ç¿»è¯‘çš„è¡Œï¼ˆå®Œå…¨ç©ºçš„å’Œéƒ¨åˆ†ç©ºçš„ï¼‰
- æ¯æ¬¡LLMç¿»è¯‘20è¡Œï¼Œ5ä¸ªLLMå¹¶å‘
- å•æ‰¹æ¬¡å¤±è´¥é‡è¯•2æ¬¡ï¼Œå¤±è´¥åç»§ç»­åç»­æ‰¹æ¬¡
- ç¡®ä¿190è¡Œéƒ½å®Œæˆç¿»è¯‘
- è¾“å‡ºæ–‡ä»¶å = è¾“å…¥æ–‡ä»¶å + å®Œæˆæ—¶é—´æˆ³
"""

import os
import json
import pandas as pd
import asyncio
from openai import AsyncOpenAI
from dotenv import load_dotenv
import time
from datetime import datetime
import random
import argparse

load_dotenv("/mnt/d/work/trans_excel/.env")

API_KEY = os.getenv("DASHSCOPE_API_KEY")
BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
MODEL = "qwen-plus"
BATCH_SIZE = 3  # æ¯æ‰¹å¤„ç†3è¡Œ
MAX_CONCURRENT = 10  # æœ€å¤§10ä¸ªå¹¶å‘
TOTAL_ROWS = 190  # æå–å‰190è¡Œ
MAX_RETRY_ATTEMPTS = 2  # å•æ‰¹æ¬¡æœ€å¤§é‡è¯•2æ¬¡
RETRY_BASE_DELAY = 3  # é‡è¯•åŸºç¡€å»¶è¿Ÿ(ç§’)

class CompletionTranslator:
    def __init__(self):
        self.client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)
        self.semaphore = asyncio.Semaphore(MAX_CONCURRENT)
        self.completed_batches = 0
        self.total_batches = 0
        self.failed_batches = []

    def create_translation_prompt(self):
        """åˆ›å»ºç¿»è¯‘æç¤ºè¯"""
        return """ä½ æ˜¯ä¸“ä¸šçš„å¤šè¯­è¨€ç¿»è¯‘ä¸“å®¶ã€‚å°†ä¸­æ–‡ç¿»è¯‘æˆè‘¡è„ç‰™è¯­ã€æ³°è¯­ã€å°å°¼è¯­ã€‚

è¿”å›JSONæ ¼å¼ï¼š
{
  "translations": [
    {
      "id": "text_0",
      "original_text": "åŸæ–‡",
      "pt": "è‘¡è„ç‰™è¯­ç¿»è¯‘",
      "th": "æ³°è¯­ç¿»è¯‘",
      "ind": "å°å°¼è¯­ç¿»è¯‘"
    }
  ]
}

è¦æ±‚ï¼š
1. ä¿æŒåŸæ–‡çš„è¯­ä¹‰å’Œè¯­æ°”
2. ä½¿ç”¨å‡†ç¡®çš„æœ¯è¯­ç¿»è¯‘
3. è¿”å›çº¯JSONæ ¼å¼ï¼Œä¸è¦å…¶ä»–å†…å®¹"""

    async def translate_batch_with_retry(self, batch_data, batch_id):
        """å¸¦é‡è¯•æœºåˆ¶çš„å¼‚æ­¥ç¿»è¯‘å•ä¸ªæ‰¹æ¬¡"""
        async with self.semaphore:
            start_time = time.time()
            print(f"ğŸ“¦ æ‰¹æ¬¡{batch_id}: å¼€å§‹å¤„ç† {len(batch_data)}è¡Œ [å¹¶å‘æ§½ä½å·²å ç”¨]")

            # å‡†å¤‡è¾“å…¥æ•°æ®
            input_texts = []
            for i, (_, text) in enumerate(batch_data):
                input_texts.append({
                    "id": f"batch_{batch_id}_text_{i}",
                    "text": text
                })

            # é‡è¯•æœºåˆ¶ - æœ€å¤š2æ¬¡é‡è¯•
            for attempt in range(MAX_RETRY_ATTEMPTS + 1):
                try:
                    # æ·»åŠ éšæœºå»¶è¿Ÿé¿å…åŒæ—¶é‡è¯•
                    if attempt > 0:
                        retry_delay = RETRY_BASE_DELAY * (2 ** (attempt - 1)) + random.uniform(0, 2)
                        print(f"ğŸ”„ æ‰¹æ¬¡{batch_id}: ç¬¬{attempt + 1}æ¬¡å°è¯•ï¼Œå»¶è¿Ÿ{retry_delay:.1f}såé‡è¯•...")
                        await asyncio.sleep(retry_delay)

                    # æ„å»ºè¯·æ±‚æ¶ˆæ¯
                    messages = [
                        {"role": "system", "content": self.create_translation_prompt()},
                        {"role": "user", "content": f"è¯·ç¿»è¯‘ä»¥ä¸‹ä¸­æ–‡æ–‡æœ¬ï¼š\n{json.dumps(input_texts, ensure_ascii=False, indent=2)}"}
                    ]

                    # æ˜¾ç¤ºè¯·æ±‚åŸå§‹æ•°æ®
                    print(f"ğŸ”— æ‰¹æ¬¡{batch_id} APIè¯·æ±‚åŸå§‹æ•°æ®:")
                    print(f"   æ¨¡å‹: {MODEL}")
                    print(f"   æ¶ˆæ¯æ•°: {len(messages)}")
                    print(f"   System prompt: {messages[0]['content']}")
                    print(f"   User content : {messages[1]['content']}")
                    print()

                    # è°ƒç”¨API
                    response = await self.client.chat.completions.create(
                        model=MODEL,
                        messages=messages,
                        temperature=0.3,
                        max_tokens=4000,
                        response_format={"type": "json_object"},
                        timeout=90  # å¢åŠ è¶…æ—¶è®¾ç½®
                    )

                    # æ˜¾ç¤ºå“åº”åŸå§‹æ•°æ®
                    print(f"ğŸ“¥ æ‰¹æ¬¡{batch_id} APIå“åº”åŸå§‹æ•°æ®:")
                    print(f"   çŠ¶æ€: æˆåŠŸ")
                    print(f"   æ¨¡å‹: {response.model}")
                    print(f"   Tokenä½¿ç”¨: {response.usage.total_tokens} (prompt: {response.usage.prompt_tokens}, completion: {response.usage.completion_tokens})")
                    print(f"   å“åº”å†…å®¹ (å‰300å­—ç¬¦): {response.choices[0].message.content[:300]}...")
                    print()

                    result = json.loads(response.choices[0].message.content)
                    translations = result.get("translations", [])

                    # å¤„ç†ç»“æœ
                    batch_results = {}
                    for i, translation in enumerate(translations):
                        if i < len(batch_data):
                            original_idx = batch_data[i][0]
                            batch_results[original_idx] = {
                                'pt': translation.get('pt', ''),
                                'th': translation.get('th', ''),
                                'ind': translation.get('ind', '')
                            }

                    elapsed = time.time() - start_time
                    self.completed_batches += 1

                    retry_info = f" (ç¬¬{attempt + 1}æ¬¡å°è¯•)" if attempt > 0 else ""
                    print(f"âœ… æ‰¹æ¬¡{batch_id}: å®Œæˆç¿»è¯‘{retry_info} {len(translations)}æ¡ | "
                          f"è€—æ—¶{elapsed:.1f}s | tokens: {response.usage.total_tokens} | "
                          f"è¿›åº¦: {self.completed_batches}/{self.total_batches}")

                    return batch_results

                except Exception as e:
                    elapsed = time.time() - start_time

                    if attempt < MAX_RETRY_ATTEMPTS:
                        print(f"âš ï¸ æ‰¹æ¬¡{batch_id}: ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥ | é”™è¯¯: {e}")
                    else:
                        print(f"âŒ æ‰¹æ¬¡{batch_id}: é‡è¯•{MAX_RETRY_ATTEMPTS}æ¬¡åæœ€ç»ˆå¤±è´¥ | æ€»è€—æ—¶{elapsed:.1f}s | é”™è¯¯: {e}")
                        self.failed_batches.append(batch_id)

            return {}  # é‡è¯•å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœï¼Œç»§ç»­å¤„ç†åç»­æ‰¹æ¬¡

def load_smart_data(file_path):
    """æ™ºèƒ½åŠ è½½æ•°æ® - æ”¯æŒä¸åŒç±»å‹çš„æ–‡ä»¶"""

    print(f"ğŸ” æ™ºèƒ½æ–‡ä»¶æ£€æµ‹: {file_path}")

    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    df = pd.read_excel(file_path)
    print(f"ğŸ“ åŠ è½½æ–‡ä»¶: {file_path}")
    print(f"   æ€»è¡Œæ•°: {len(df)}è¡Œ")
    print(f"   åˆ—å: {list(df.columns)}")

    # ç¡®ä¿å–å‰190è¡Œ
    df = df.head(TOTAL_ROWS).copy()
    print(f"   å¤„ç†æ•°æ®: {len(df)}è¡Œ")

    # ç¡®ä¿ç¿»è¯‘åˆ—å­˜åœ¨
    for col in ['PT', 'TH', 'IND']:
        if col not in df.columns:
            df[col] = ''

    # åˆ†æç¿»è¯‘çŠ¶æ€
    def count_translated(column):
        if column not in df.columns:
            return 0
        return df[column].notna().sum() - (df[column] == '').sum()

    pt_count = count_translated('PT')
    th_count = count_translated('TH')
    ind_count = count_translated('IND')
    total_translated = pt_count + th_count + ind_count
    total_possible = len(df) * 3

    print(f"   ğŸ“Š ç¿»è¯‘çŠ¶æ€åˆ†æ:")
    print(f"     è‘¡è„ç‰™è¯­(PT): {pt_count}/{len(df)} è¡Œ")
    print(f"     æ³°è¯­(TH): {th_count}/{len(df)} è¡Œ")
    print(f"     å°å°¼è¯­(IND): {ind_count}/{len(df)} è¡Œ")
    print(f"     æ€»å®Œæˆåº¦: {total_translated}/{total_possible} ({total_translated/total_possible*100:.1f}%)")

    if total_translated == total_possible:
        print("   âœ… æ‰€æœ‰ç¿»è¯‘å·²å®Œæˆï¼")
        return df, []
    else:
        print(f"   âš ï¸  å‘ç° {total_possible - total_translated} ä¸ªç¼ºå¤±ç¿»è¯‘ï¼Œå°†è¿›è¡Œè¡¥å……")
        return df, find_missing_translations(df)

def find_missing_translations(df):
    """æ‰¾å‡ºéœ€è¦ç¿»è¯‘çš„è¡Œ"""
    def is_translated(value):
        return pd.notna(value) and str(value).strip() != '' and str(value) != 'nan'

    missing_translations = []

    for idx, row in df.iterrows():
        pt_missing = not is_translated(row.get('PT'))
        th_missing = not is_translated(row.get('TH'))
        ind_missing = not is_translated(row.get('IND'))

        # å¦‚æœä»»ä½•ä¸€ä¸ªè¯­è¨€ç¼ºå¤±ï¼Œå°±éœ€è¦é‡æ–°ç¿»è¯‘è¯¥è¡Œ
        if pt_missing or th_missing or ind_missing:
            missing_translations.append({
                'index': idx,
                'chinese': row['CH'],
                'missing_languages': {
                    'pt': pt_missing,
                    'th': th_missing,
                    'ind': ind_missing
                }
            })

    return missing_translations

def create_translation_batches(missing_translations):
    """å°†ç¼ºå¤±ç¿»è¯‘è½¬æ¢ä¸ºæ‰¹æ¬¡æ•°æ®"""
    # æŒ‰æ‰¹æ¬¡å¤§å°åˆ†ç»„
    batches = []
    for i in range(0, len(missing_translations), BATCH_SIZE):
        batch = missing_translations[i:i + BATCH_SIZE]
        # è½¬æ¢ä¸º (index, chinese_text) æ ¼å¼
        batch_data = [(item['index'], item['chinese']) for item in batch]
        batches.append(batch_data)

    return batches

def generate_output_filename(input_file):
    """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å: æ¸…ç†åŸºç¡€åç§° + å®Œæˆæ—¶é—´æˆ³"""
    # è·å–æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„å’Œæ‰©å±•åï¼‰
    base_name = os.path.splitext(os.path.basename(input_file))[0]

    # æ¸…ç†åŸºç¡€åç§°ï¼Œç§»é™¤ä¹‹å‰çš„completedå’Œæ—¶é—´æˆ³éƒ¨åˆ†
    # åŒ¹é…æ¨¡å¼ï¼š_completed_YYYYMMDD_HHMMSS
    import re
    clean_base = re.sub(r'_completed_\d{8}_\d{6}', '', base_name)

    # å¦‚æœæ¸…ç†åä¸ºç©ºæˆ–è€…å¤ªçŸ­ï¼Œä½¿ç”¨åŸå§‹åç§°çš„å‰éƒ¨åˆ†
    if len(clean_base) < 3:
        # å–åŸå§‹åç§°çš„å‰é¢éƒ¨åˆ†ï¼Œæœ€å¤š20ä¸ªå­—ç¬¦
        clean_base = base_name.split('_')[0][:20]

    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    output_filename = f"{clean_base}_completed_{timestamp}.xlsx"

    return output_filename

async def run_translation(missing_translations, translator):
    """æ‰§è¡Œç¿»è¯‘ä»»åŠ¡"""
    if not missing_translations:
        print("ğŸ‰ æ‰€æœ‰ç¿»è¯‘éƒ½å·²å®Œæˆï¼Œæ— éœ€è¿›ä¸€æ­¥å¤„ç†ï¼")
        return {}

    translation_batches = create_translation_batches(missing_translations)

    print(f"ğŸš€ å¼€å§‹æ™ºèƒ½ç¿»è¯‘...")
    print(f"   ğŸ“Š éœ€è¦ç¿»è¯‘è¡Œæ•°: {len(missing_translations)}")
    print(f"   ğŸ“¦ åˆ†æˆæ‰¹æ¬¡æ•°: {len(translation_batches)}")
    print()

    # æ‰‹åŠ¨å¤„ç†æ‰¹æ¬¡
    all_translations = {}
    translator.total_batches = len(translation_batches)

    # æ‰§è¡Œç¿»è¯‘
    start_time = time.time()
    print(f"ğŸ¯ {datetime.now().strftime('%H:%M:%S')} å¼€å§‹ç¿»è¯‘...")

    tasks = []
    for batch_id, batch_data in enumerate(translation_batches, 1):
        tasks.append(translator.translate_batch_with_retry(batch_data, batch_id))

    results = await asyncio.gather(*tasks, return_exceptions=True)

    # åˆå¹¶ç»“æœ
    for result in results:
        if isinstance(result, dict):
            all_translations.update(result)

    elapsed = time.time() - start_time
    successful_batches = sum(1 for r in results if isinstance(r, dict) and r)

    print()
    print(f"ğŸ {datetime.now().strftime('%H:%M:%S')} ç¿»è¯‘å®Œæˆ!")
    print(f"   â±ï¸  æ€»è€—æ—¶: {elapsed:.1f}ç§’")
    print(f"   ğŸ“Š æˆåŠŸæ‰¹æ¬¡: {successful_batches}/{len(translation_batches)}")
    print(f"   ğŸ“ æ–°å¢ç¿»è¯‘: {len(all_translations)}è¡Œ")

    return all_translations

async def test_llm_connection():
    """æµ‹è¯•LLMè¿æ¥"""
    print("ğŸ”§ æµ‹è¯•LLMè¿æ¥...")

    try:
        client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)

        # æµ‹è¯•ç®€å•è¯·æ±‚
        response = await client.chat.completions.create(
            model=MODEL,
            messages=[{"role": "user", "content": "hello"}],
            max_tokens=50
        )

        print(f"âœ… LLMè¿æ¥æµ‹è¯•æˆåŠŸ:")
        print(f"   æ¨¡å‹: {response.model}")
        print(f"   å“åº”: {response.choices[0].message.content}")
        print(f"   Tokenä½¿ç”¨: {response.usage.total_tokens} (prompt: {response.usage.prompt_tokens}, completion: {response.usage.completion_tokens})")
        print()

        return True

    except Exception as e:
        print(f"âŒ LLMè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='æ™ºèƒ½å¹¶å‘ç¿»è¯‘å·¥å…·')
    parser.add_argument('-f', '--file', required=True, help='è¾“å…¥Excelæ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()

    input_file = args.file

    print("ğŸ”µ æ™ºèƒ½å¹¶å‘ç¿»è¯‘ - ç®€åŒ–ç‰ˆ")
    print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   ç›®æ ‡è¡Œæ•°: {TOTAL_ROWS}")
    print(f"   æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}è¡Œ/æ‰¹æ¬¡")
    print(f"   å¹¶å‘æ•°: {MAX_CONCURRENT}")
    print(f"   é‡è¯•æ¬¡æ•°: {MAX_RETRY_ATTEMPTS}")
    print(f"   æ¨¡å‹: {MODEL}")
    print(f"   APIåœ°å€: {BASE_URL}")
    print(f"   è¾“å…¥æ–‡ä»¶: {input_file}")
    print("=" * 70)

    # å…ˆæµ‹è¯•LLMè¿æ¥
    print("ğŸ”§ åˆå§‹åŒ–æµ‹è¯•...")
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        connection_ok = loop.run_until_complete(test_llm_connection())
        if not connection_ok:
            print("âŒ LLMè¿æ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æ‰§è¡Œç¿»è¯‘ä»»åŠ¡")
            return
    finally:
        loop.close()

    try:
        # æ™ºèƒ½åŠ è½½æ•°æ®
        df, missing_translations = load_smart_data(input_file)

        if not missing_translations:
            print("ğŸ‰ æ‰€æœ‰ç¿»è¯‘éƒ½å·²å®Œæˆï¼Œæ— éœ€è¿›ä¸€æ­¥å¤„ç†ï¼")
            return

        print(f"ğŸ“Š ç¿»è¯‘ä»»åŠ¡åˆ†æ:")
        print(f"   æ•°æ®æ€»è¡Œæ•°: {len(df)}")
        print(f"   éœ€è¦ç¿»è¯‘: {len(missing_translations)}è¡Œ")
        print()

        # æ˜¾ç¤ºéœ€è¦ç¿»è¯‘çš„è¡Œï¼ˆå‰5è¡Œç¤ºä¾‹ï¼‰
        print("ğŸ“‹ éœ€è¦ç¿»è¯‘çš„è¡Œ (å‰5è¡Œç¤ºä¾‹):")
        for i, item in enumerate(missing_translations[:5]):
            missing_langs = [lang for lang, is_missing in item['missing_languages'].items() if is_missing]
            print(f"   è¡Œ{item['index']+1}: {item['chinese']} -> ç¼ºå¤±: {', '.join(missing_langs)}")

        if len(missing_translations) > 5:
            print(f"   ... è¿˜æœ‰ {len(missing_translations) - 5} è¡Œéœ€è¦ç¿»è¯‘")
        print()

        # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
        has_content = df['CH'].notna() & (df['CH'].astype(str).str.strip() != '')
        valid_rows = has_content.sum()

        if valid_rows == 0:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„ä¸­æ–‡æ•°æ®éœ€è¦ç¿»è¯‘")
            return

        # æ‰§è¡Œå¹¶å‘ç¿»è¯‘
        translator = CompletionTranslator()

        # ä½¿ç”¨asyncioè¿è¡Œå¼‚æ­¥ä»»åŠ¡
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            translations = loop.run_until_complete(run_translation(missing_translations, translator))
        finally:
            loop.close()

        # å†™å›ç»“æœ
        print(f"ğŸ’¾ æ›´æ–°ç¿»è¯‘ç»“æœ...")
        result_df = df.copy()

        # ç¡®ä¿åˆ—å­˜åœ¨ä¸”ä¸ºæ­£ç¡®ç±»å‹
        for col in ['PT', 'TH', 'IND']:
            if col not in result_df.columns:
                result_df[col] = ''
            result_df[col] = result_df[col].astype('object')

        # å¡«å……æ–°ç¿»è¯‘
        filled_count = 0
        for idx, translation in translations.items():
            if idx < len(result_df):
                result_df.at[idx, 'PT'] = translation.get('pt', '')
                result_df.at[idx, 'TH'] = translation.get('th', '')
                result_df.at[idx, 'IND'] = translation.get('ind', '')
                filled_count += 1

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        output_path = generate_output_filename(input_file)
        result_df.to_excel(output_path, index=False)

        # æœ€ç»ˆç»Ÿè®¡
        def count_final_translated(column):
            return result_df[column].notna().sum() - (result_df[column] == '').sum()

        final_pt = count_final_translated('PT')
        final_th = count_final_translated('TH')
        final_ind = count_final_translated('IND')
        total_final = final_pt + final_th + final_ind
        total_possible = len(result_df) * 3

        print(f"âœ… æ™ºèƒ½ç¿»è¯‘å®Œæˆ!")
        print(f"   ğŸ“„ è¾“å‡ºæ–‡ä»¶: {output_path}")
        print(f"   ğŸ“Š æœ¬æ¬¡æ–°å¢: {filled_count * 3}ä¸ªç¿»è¯‘")
        print(f"   ğŸ¯ æœ€ç»ˆå®Œæˆåº¦: {total_final}/{total_possible} ({total_final/total_possible*100:.1f}%)")
        print()

        # æ˜¾ç¤ºç¿»è¯‘ç¤ºä¾‹
        if filled_count > 0:
            print("ğŸ“‹ ç¿»è¯‘ç¤ºä¾‹ (å‰3è¡Œ):")
            translated_indices = list(translations.keys())
            sample_count = min(3, len(translated_indices))

            for i in range(sample_count):
                idx = translated_indices[i]
                row = result_df.iloc[idx]
                print(f"   è¡Œ{idx+1}. ä¸­æ–‡: {row['CH']}")
                print(f"      è‘¡è¯­: {row['PT']}")
                print(f"      æ³°è¯­: {row['TH']}")
                print(f"      å°å°¼: {row['IND']}")
                print()

        print(f"ğŸ‰ å¤„ç†å®Œæˆ! è¾“å‡ºæ–‡ä»¶: {output_path}")

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()